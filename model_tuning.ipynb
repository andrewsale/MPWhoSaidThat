{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08bbc782",
   "metadata": {},
   "source": [
    "# Model architecture\n",
    "\n",
    "The model is built in the function `NN` below. The model can be customized with various arguments, but the general structure is as follows:\n",
    "\n",
    "* **Text Vectorizer**: this encodes the input string to a sequence of integers. The layer will produce sequences of length up to 100, and each *word* is encoded as an integer. Hence input strings should ideally have length at most 100 to avoid lost information. The size of the vocabulary can be customized.\n",
    "* **Embedding layer**: the input sequences are embedded into $n$-dimensional space. You may choose $n$, and you may also try using embeddings pretrained using GloVe (for `embed_layer` enter either \"glove 50\" or \"glove 50 trainable\" for the 50 dimensional embeddings, making this layer either trainable or not. GloVe embeddings may be 50, 100, 200 or 300 dimensional.)\n",
    "* **Dropout layer**: the dropout rate can be set with by `Dropout_rate`. The same rate is used for all dropout layers.\n",
    "* **Convolutional layers**: by default none are used. The number of filters and filter size can be customized. If used, the `Conv1D` layer is followed by a **max pooling** layer (the pooling size can be customized) and a **dropout layer**.\n",
    "* **LSTM layers**: you may choose how many layers, how many unites and whether to use kernel regularization. You may also choose whether you want the layers to be bidirectional or not (with the `BRNN` argument).\n",
    "* **Dropout layer**\n",
    "* **Dense layers**: you may choose how many layers to use, and how many neurons in each layer. You may also set the regularization and whether to use batch normalization.\n",
    "* **Dropout layer**\n",
    "* **Output layer**: one neuron with sigmoid activation to output classification probabilities.\n",
    "\n",
    "The `TextVectorizer` layer needs to be initialized first, and is done so with the `vect_layer_maker` function. It is here where you can choose the vocabulary size.\n",
    "\n",
    "If using the GloVe embeddings, then these need to be initialized too. Use the `get_glove_embedding_layer` to do this, which returns a pretrained embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39166f7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-10-19T00:33:21.305370Z",
     "iopub.status.busy": "2022-10-19T00:33:21.304980Z",
     "iopub.status.idle": "2022-10-19T00:33:31.591528Z",
     "shell.execute_reply": "2022-10-19T00:33:31.590084Z"
    },
    "papermill": {
     "duration": 10.307285,
     "end_time": "2022-10-19T00:33:31.594093",
     "exception": false,
     "start_time": "2022-10-19T00:33:21.286808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Activation, BatchNormalization, TextVectorization, Dropout, Bidirectional, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from time import time\n",
    "from functools import partial\n",
    "from zipfile import ZipFile\n",
    "import re\n",
    "from helper_functions import * # functions defined in helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068fa54",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-19T00:33:32.330272Z",
     "iopub.status.busy": "2022-10-19T00:33:32.329919Z",
     "iopub.status.idle": "2022-10-19T00:33:32.335712Z",
     "shell.execute_reply": "2022-10-19T00:33:32.334833Z"
    },
    "papermill": {
     "duration": 0.024757,
     "end_time": "2022-10-19T00:33:32.337468",
     "exception": false,
     "start_time": "2022-10-19T00:33:32.312711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vect_layer_maker(X_train, vocab_size=10000, sequence_length=100):\n",
    "    vectorize_layer = TextVectorization(max_tokens = vocab_size,\n",
    "                                        output_mode='int',\n",
    "                                        output_sequence_length=100,\n",
    "                                        name='vectorizer'\n",
    "                                       )\n",
    "    vectorize_layer.adapt(X_train)\n",
    "    return vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6242cc7",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-19T00:33:32.404055Z",
     "iopub.status.busy": "2022-10-19T00:33:32.403019Z",
     "iopub.status.idle": "2022-10-19T00:33:32.410970Z",
     "shell.execute_reply": "2022-10-19T00:33:32.410322Z"
    },
    "papermill": {
     "duration": 0.026911,
     "end_time": "2022-10-19T00:33:32.412524",
     "exception": false,
     "start_time": "2022-10-19T00:33:32.385613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_glove_embedding_layer(vectorizer_layer, embed_dim=300, trainable=False):\n",
    "    # First get the vectors            \n",
    "    glove_dict = {}\n",
    "    if embed_dim not in [50,100,200,300]:\n",
    "        raise ValueError('Glove embedding dimensions are 50, 100, 200 or 300 only.')\n",
    "    filename = f'../input/glove6b/glove.6B.{embed_dim}d.txt'\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = values[1:]\n",
    "            glove_dict[word] = np.asarray(vector, dtype='float32')\n",
    "    # Create vocab dictionary from the vectorizer layer\n",
    "    vocab_list = vectorizer_layer.get_vocabulary()\n",
    "    vocab_dict = {word : index for index, word in enumerate(vocab_list)}\n",
    "    # Next create the matrix\n",
    "    embedding_matrix = np.zeros((len(vocab_dict)+1, embed_dim))\n",
    "    for word, index in vocab_dict.items():\n",
    "        embedding_vector = glove_dict.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    # Now create and return the layer\n",
    "    return Embedding(input_dim = len(vocab_dict)+1,\n",
    "                     output_dim = embed_dim,\n",
    "                     trainable = trainable,\n",
    "                     weights = [embedding_matrix]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5929b8ca",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-19T00:33:32.247878Z",
     "iopub.status.busy": "2022-10-19T00:33:32.247146Z",
     "iopub.status.idle": "2022-10-19T00:33:32.262898Z",
     "shell.execute_reply": "2022-10-19T00:33:32.262048Z"
    },
    "papermill": {
     "duration": 0.034569,
     "end_time": "2022-10-19T00:33:32.264473",
     "exception": false,
     "start_time": "2022-10-19T00:33:32.229904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def NN(embed_layer = 32,\n",
    "       Conv_filters=None,\n",
    "       Conv_filter_size=3,\n",
    "       Conv_pool_size=2,\n",
    "       LSTM_layers=2, \n",
    "       LSTM_units=64, \n",
    "       LSTM_kernel_reg=None,\n",
    "       BRNN=False,\n",
    "       Dense_layers=1, \n",
    "       Dense_units=128, \n",
    "       Dense_reg=None,\n",
    "       Batch_norm=False,\n",
    "       Dropout_rate=0,\n",
    "       learning_rate=0.001\n",
    "      ):\n",
    "    '''\n",
    "    Builds and compiles RNN.\n",
    "    \n",
    "    Args:\n",
    "        embed_layer: int (dimension of embedding) or pre-built keras layer\n",
    "        LSTM_layers: int  -- number of LSTM layers, come after the embedding layer\n",
    "        LSTM_units: int\n",
    "        LSTM_kernel_reg:  tf.keras.regularizers object for regularization of input weights\n",
    "        Dense_layers: int  -- number of Dense layers afer LSTM\n",
    "        Dense_units: int\n",
    "        Dense_reg: tf.keras.regularizers object for regularization of weights\n",
    "        Dropout_rate: float (default 0 = no dropout)  -- dropout rate for dropout layers\n",
    "        \n",
    "    Returns:\n",
    "        compiled model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    # vectorization layer\n",
    "    model.add(vectorize_layer)\n",
    "    # Embedding layer\n",
    "    if isinstance(embed_layer, int):\n",
    "        embed_layer = Embedding(vocab_size+1, embed_layer)\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Input(shape=[100], dtype=tf.string))\n",
    "        if embed_layer in [f'glove {x}' for x in [50,100,200,300]]:\n",
    "            embed_dim = int(re.findall(r'\\d+', embed_layer)[0])\n",
    "            embed_layer = get_glove_embedding_layer(vectorize_layer, embed_dim=embed_dim, trainable=False)\n",
    "        elif embed_layer in [f'glove {x} trainable' for x in [50,100,200,300]]:\n",
    "            embed_dim = int(re.findall(r'\\d+', embed_layer)[0])\n",
    "            embed_layer = get_glove_embedding_layer(vectorize_layer, embed_dim=embed_dim, trainable=True)\n",
    "        else:\n",
    "            raise ValueError('Embedding layer format not recognized. Should be either integer for trainable embedding of that dimension, or either \"glove x\" or \"glove x trainable\", for x = 50, 100, 200, 300')\n",
    "    model.add(embed_layer)\n",
    "    # Dropout layer\n",
    "    model.add(Dropout(Dropout_rate))\n",
    "    # Add Conv layer (optional)\n",
    "    if Conv_filters != None:\n",
    "        model.add(Conv1D(filters=Conv_filters, kernel_size=Conv_filter_size, padding = 'same', activation='relu', name='Conv1D'))\n",
    "        model.add(MaxPooling1D(pool_size=Conv_pool_size, name='MaxPool'))\n",
    "        model.add(Dropout(Dropout_rate))\n",
    "    # add LSTM layers\n",
    "    if BRNN:\n",
    "        if LSTM_layers>1:\n",
    "            for i in range(LSTM_layers-1):\n",
    "                model.add(Bidirectional(LSTM(LSTM_units, return_sequences = True, kernel_regularizer = LSTM_kernel_reg, name = f'LSTM_{i}')))\n",
    "        else:\n",
    "            i=0\n",
    "        model.add(Bidirectional(LSTM(LSTM_units, name = f'LSTM_{i+1}')))\n",
    "        \n",
    "    else:\n",
    "        if LSTM_layers>1:\n",
    "            for i in range(LSTM_layers-1):\n",
    "                model.add(LSTM(LSTM_units, return_sequences = True, kernel_regularizer = LSTM_kernel_reg, name = f'LSTM_{i}'))\n",
    "        else:\n",
    "            i=0\n",
    "        model.add(LSTM(LSTM_units, name = f'LSTM_{i+1}'))\n",
    "    # Another dropout layer\n",
    "    model.add(Dropout(Dropout_rate))\n",
    "    # add fully connected layers\n",
    "    for i in range(Dense_layers):\n",
    "        model.add(Dense(Dense_units, kernel_initializer = 'he_uniform', activation=None, kernel_regularizer=Dense_reg, name = f'Dense_{i}'))\n",
    "        if Batch_norm:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "    # Another dropout layer\n",
    "    model.add(Dropout(Dropout_rate))\n",
    "    # add final layer, outputting proability speech is Tory\n",
    "    model.add(Dense(1, activation='sigmoid', name = 'output_layer'))\n",
    "    # compile\n",
    "    Adam = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b14bd2",
   "metadata": {
    "papermill": {
     "duration": 0.015873,
     "end_time": "2022-10-19T00:33:32.445097",
     "exception": false,
     "start_time": "2022-10-19T00:33:32.429224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The grid search begins..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1984f151",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-19T00:33:32.479522Z",
     "iopub.status.busy": "2022-10-19T00:33:32.478577Z",
     "iopub.status.idle": "2022-10-19T00:33:37.492483Z",
     "shell.execute_reply": "2022-10-19T00:33:37.491080Z"
    },
    "papermill": {
     "duration": 5.033797,
     "end_time": "2022-10-19T00:33:37.494923",
     "exception": false,
     "start_time": "2022-10-19T00:33:32.461126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = load_data(batch_size=512) # load the data\n",
    "\n",
    "vocab_size=30000\n",
    "vectorize_layer = vect_layer_maker(train_ds.map(lambda x,y:x), vocab_size=vocab_size, sequence_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1089837",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-10-19T00:33:37.532943Z",
     "iopub.status.busy": "2022-10-19T00:33:37.531898Z",
     "iopub.status.idle": "2022-10-19T03:08:28.085412Z",
     "shell.execute_reply": "2022-10-19T03:08:28.084575Z"
    },
    "papermill": {
     "duration": 9290.57508,
     "end_time": "2022-10-19T03:08:28.087959",
     "exception": false,
     "start_time": "2022-10-19T00:33:37.512879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg = l2(0.01)\n",
    "reg2 = l2(0.1)\n",
    "n_repeats = 1\n",
    "history_log = grid_search(NN, train_ds, val_ds, n_repeats = n_repeats, n_epochs=10, early_stopping_patience=6,verbose=1,\n",
    "                                   embed_layer = [16],\n",
    "                                   Conv_filters=[16],\n",
    "                                   Conv_filter_size=[5],\n",
    "                                   Conv_pool_size=[2],\n",
    "                                   LSTM_layers=[2], \n",
    "                                   LSTM_units=[32], \n",
    "                                   LSTM_kernel_reg=[reg2],\n",
    "                                   BRNN=[True],\n",
    "                                   Dense_layers=[2], \n",
    "                                   Dense_units=[64], \n",
    "                                   Dense_reg=[reg2],\n",
    "                                   Batch_norm=[True],\n",
    "                                   Dropout_rate=[0.3],\n",
    "                                   learning_rate=[0.0001]\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445977a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T03:08:31.316522Z",
     "iopub.status.busy": "2022-10-19T03:08:31.316088Z",
     "iopub.status.idle": "2022-10-19T03:08:36.463332Z",
     "shell.execute_reply": "2022-10-19T03:08:36.462350Z"
    },
    "papermill": {
     "duration": 6.751348,
     "end_time": "2022-10-19T03:08:36.465456",
     "exception": false,
     "start_time": "2022-10-19T03:08:29.714108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_log(history_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ca8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9419.54824,
   "end_time": "2022-10-19T03:09:14.218672",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-19T00:32:14.670432",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "596478c17f57cc447248f5b701a09fe45984fb1f5846d2cd98bab2f18ccdeeec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
