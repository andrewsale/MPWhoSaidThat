{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Speeches by UK Members of Pariament\n","\n","Other notebooks in this series:\n","* <a href='https://www.kaggle.com/code/andrewsale/speech-scraping'>Scraping notebook</a>, used to create <a href='https://www.kaggle.com/datasets/andrewsale/uk-political-speeches'>the dataset</a>.\n","* <a href='https://www.kaggle.com/code/andrewsale/speeches-data-wrangling'>Wrangling notebook</a>, tidying up the dataset.\n","* <a href='https://www.kaggle.com/code/andrewsale/speeches-sampling'>Sampling notebook</a>, creating the short speech samples used for training the model in this notebook.\n","\n","## The model\n","\n","The aim is to build an app in which you can enter the text of a real speech given by a politician, or make up your own. The app will then predict if the speech was given by someone from the left-leaning Labour Party, or the right-leaning Conservative Party.\n","\n","The model behind the app is constructed using Tensorflow. However, since it will be exported to TensorflowJS, we are limited as to which layers we can include.\n","\n","The app is available <a href=\"https://andrewsale.github.io/MP-app.github.io/\">here</a>, and there are two Github repositories: <a href=\"https://github.com/andrewsale/MP-app.github.io\">one for the app</a>, and <a href=\"https://github.com/andrewsale/MPWhoSaidThat\">another for the model building</a>."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-11-26T10:07:21.729458Z","iopub.status.busy":"2022-11-26T10:07:21.727827Z","iopub.status.idle":"2022-11-26T10:07:31.528903Z","shell.execute_reply":"2022-11-26T10:07:31.527411Z","shell.execute_reply.started":"2022-11-26T10:07:21.729287Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras import layers\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.regularizers import l2\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, log_loss\n","from time import time\n","import re\n","import json"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing: tokenizing the text\n","\n","The keras tokenizer will convert to lower case and remove punctuation before splitting on white space and converting words to integers. We fit the tokenizer to the full training set speech text (not the samples), limiting the vocabulary to 30,000 words.\n","\n","We then tokenize the speech samples, loaded from the <a href='https://www.kaggle.com/code/andrewsale/speeches-sampling'>sampling notebook</a>. These samples are about 100 words each, and we use the `pad_sequences` functions from keras to pad or truncate the sequences to this length.\n","\n","### A note on other tokenizing techniques\n","\n","There are other effective tokenizing techniques, such as <a href=\"https://www.tensorflow.org/text/guide/subwords_tokenizer\">subword tokenizing</a>, however we needed a simple tokenizing method that could be reproduced in javasript."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-11-26T10:07:31.532713Z","iopub.status.busy":"2022-11-26T10:07:31.531177Z","iopub.status.idle":"2022-11-26T10:07:35.776124Z","shell.execute_reply":"2022-11-26T10:07:35.774455Z","shell.execute_reply.started":"2022-11-26T10:07:31.532647Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# Load the trianing data to initialize the tokenizer\n","full_train_df = pd.read_csv('../input/speeches-sampling/full_train.csv')\n","# Load the sampled speeches for conversion\n","train_df = pd.read_csv('../input/speeches-sampling/train.csv')\n","val_df = pd.read_csv('../input/speeches-sampling/val.csv')\n","test_df = pd.read_csv('../input/speeches-sampling/test.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-26T10:07:35.778454Z","iopub.status.busy":"2022-11-26T10:07:35.778026Z","iopub.status.idle":"2022-11-26T10:07:51.832048Z","shell.execute_reply":"2022-11-26T10:07:51.830585Z","shell.execute_reply.started":"2022-11-26T10:07:35.778423Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["vocab_size=30000\n","# Instantiate the tokenizer using the training data\n","tokenizer = Tokenizer(num_words=vocab_size)\n","tokenizer.fit_on_texts(full_train_df['Speech'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-26T10:07:51.834713Z","iopub.status.busy":"2022-11-26T10:07:51.834253Z","iopub.status.idle":"2022-11-26T10:08:08.065008Z","shell.execute_reply":"2022-11-26T10:08:08.063927Z","shell.execute_reply.started":"2022-11-26T10:07:51.834581Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# Convert speeches to sequences of tokens and put into tf dataset\n","BATCH_SIZE = 128\n","def tokenize_to_ds(df, batch_size=BATCH_SIZE):\n","    tokenized = tokenizer.texts_to_sequences(df.Speech)\n","    tokenized = pad_sequences(tokenized,maxlen=100)\n","    labels = df.Label.tolist()\n","    return tf.data.Dataset.from_tensor_slices((tokenized, labels)).cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","train_ds = tokenize_to_ds(train_df)\n","val_ds = tokenize_to_ds(val_df)\n","test_ds = tokenize_to_ds(test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-26T10:08:08.080515Z","iopub.status.busy":"2022-11-26T10:08:08.079724Z","iopub.status.idle":"2022-11-26T10:08:08.140201Z","shell.execute_reply":"2022-11-26T10:08:08.138683Z","shell.execute_reply.started":"2022-11-26T10:08:08.080458Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# Create the vocabulary dictionary.\n","# Note that the word_index attribute includes ALL words the tokenizer saw\n","# when fitting, even though vocab size was limited.\n","vocab = {}\n","for word, index in tokenizer.word_index.items():\n","    if index <= vocab_size:\n","        vocab[word] = index\n","        \n","# Write the vocab dictionary to a json file for loading in javascript\n","with open('tokenizer_dictionary.json', 'w') as file:    \n","    json.dump(vocab, file)"]},{"cell_type":"markdown","metadata":{},"source":["# Global build and test functions\n","\n","These functions are used to perform a grid search and obtain some useful performance statistics, including the performance on entire speeches in the validation set, by predicting on 100 word samples from it and aggregating the results. They also visualize the results."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-11-26T10:08:10.637956Z","iopub.status.busy":"2022-11-26T10:08:10.637597Z","iopub.status.idle":"2022-11-26T10:08:10.661813Z","shell.execute_reply":"2022-11-26T10:08:10.660353Z","shell.execute_reply.started":"2022-11-26T10:08:10.637928Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def fit_model(model_build_fn,\n","              X_train,\n","              X_val,\n","              y_train = None,\n","              y_val = None,\n","              n_repeats=1, \n","              n_epochs=10, \n","              early_stopping_patience = None,\n","              verbose=0,\n","              model_save_suffix='',\n","              **model_kwargs\n","             ):\n","    '''\n","    Takes a compiled model and data and fits it multiple times (with the same parameters).\n","    \n","    Args:\n","        model_build_fn:  function that builds and compiles TF model\n","        X_train:  input training data\n","        y_train:  training labels\n","        X_val:  input validation data\n","        y_val:  validation labels\n","        n_repeats=1: number of times to fit the model\n","        n_epochs=10, \n","        early_stopping_patience = None\n","        **model_kwargs: specify any parameters to customize for the creation of the model \n","        \n","    Returns:\n","        list of history classes\n","        \n","    File Outputs:\n","        model  --- saving the model from each run\n","    '''\n","    # Prepare dict to log histories of the runs\n","    history_log = model_kwargs.copy()\n","    history_log['Parameters'] = str(model_kwargs)\n","    history_log['Parameter keys'] = model_kwargs.keys()\n","    history_log['Number of runs'] = n_repeats\n","    run_times = []\n","    \n","    for i in range(n_repeats):        \n","        # Build model\n","        model = model_build_fn(**model_kwargs)\n","        \n","        # Establish callbacks\n","        if not early_stopping_patience == None:\n","            early_stopping_monitor = EarlyStopping(patience=early_stopping_patience)\n","#             callbacks = [early_stopping_monitor, TensorBoard(run_logdir)]\n","            callbacks = [early_stopping_monitor]\n","        else:\n","#             callbacks = [TensorBoard(run_logdir)]\n","            callbacks = []\n","        model_filename = f'model{model_save_suffix}_run_{i}'\n","        model_save = ModelCheckpoint(model_filename, \n","                                     save_best_only=True, \n","                                     save_format='tf'\n","                                    )\n","        callbacks.append(model_save)\n","    \n","        # Fit model  \n","        t0=time()\n","        history = model.fit(X_train, \n","                            y_train,\n","                            epochs=n_epochs, \n","                            validation_data=X_val, #(X_val, y_val), \n","                            callbacks=callbacks, \n","                            verbose = verbose)\n","        # Append the time for this run to the list\n","        run_times.append((time()-t0)/60)\n","        \n","        # Get vocab size\n","#         history_log['Vocab size'] = model.get_layer(name='Vectorizer').vocabulary_size()\n","        \n","        # Make preditictions on training and validation sets\n","        model = tf.keras.models.load_model(model_filename)  \n","#                                            custom_objects={\"AddPositionalEncoding\":AddPositionalEncoding,\n","#                                                            \"positional_encoding\":positional_encoding\n","#                                                           }\n","#                                          )\n","        history_log[f'y_train_pred_run_{i}'] = model.predict(X_train)\n","        history_log[f'y_val_pred_run_{i}'] = model.predict(X_val)\n","        \n","        # Get loss and accuracy on val set when using the full speeches\n","        full_val_loss, full_val_accuracy = val_performance(model, speech_segments_ds, speech_segments_df)\n","        history_log[f'y_full_val_loss_run_{i}'] = full_val_loss\n","        history_log[f'y_full_val_accuracy_run_{i}'] = full_val_accuracy\n","        \n","        # the following will extract the actual labels from a tf dataset when this is used\n","        if y_train == None:\n","            y_train1 = np.concatenate(list(X_train.map(lambda x,y:y).as_numpy_iterator()), axis=0)\n","        if y_val == None:\n","            y_val1 = np.concatenate(list(X_val.map(lambda x,y:y).as_numpy_iterator()), axis=0)\n","        history_log[f'y_train_actual'] = y_train1\n","        history_log[f'y_val_actual'] = y_val1\n","        # On the first run add the number of trainable parameters to the log dict\n","        if i==0:\n","            set_of_trainable_weights = model.trainable_weights\n","            trainable_count = int(sum([tf.keras.backend.count_params(p) for p in set_of_trainable_weights])) # counts trainable variables\n","            history_log['trainable parameters'] = trainable_count\n","            \n","        # Add the history of the current run to the log dict\n","        keys = history.history.keys()\n","        for key in keys:\n","            history_log[f'run_{i}_{key}'] = history.history[key]\n","        \n","        # Get val_loss trend over last 6 epochs\n","        actual_num_epochs = len(history.history['val_loss'])\n","        history_log['Number of epochs'] = actual_num_epochs\n","        first_epoch_for_slope = max(actual_num_epochs-6, 0)\n","        slopes = []\n","        for epoch in range(first_epoch_for_slope,actual_num_epochs):\n","            dy = history.history['val_loss'][actual_num_epochs-1] - history.history['val_loss'][first_epoch_for_slope]\n","            dx = actual_num_epochs - epoch\n","            slopes.append(dy/dx)\n","        history_log[f'Val_loss trend at end run {i}'] = np.mean(slopes)\n","    \n","    # Get mean end trend\n","    history_log[f'Val_loss mean trend at end'] = np.mean([history_log[f'Val_loss trend at end run {i}'] for i in range(n_repeats)])\n","    \n","    # Add the run time data to the log dict\n","    history_log['Mean run time (mins)'] = sum(run_times)/len(run_times)\n","    \n","    # Collect stats of the runs in the log dict\n","    for key in keys:        \n","        final_key_values = [history_log[f'run_{i}_{key}'][-1] for i in range(n_repeats)]\n","        best_key_values = [max(history_log[f'run_{i}_{key}']) for i in range(n_repeats)]\n","\n","        history_log[f'mean_{key}'] = sum(final_key_values) / n_repeats\n","        if 'loss' in key:\n","            history_log[f'best_final_{key}'] = min(final_key_values)\n","            history_log[f'best_anytime_{key}'] = min(best_key_values)            \n","        else:\n","            history_log[f'best_final_{key}'] = max(final_key_values)\n","            history_log[f'best_anytime_{key}'] = max(best_key_values)\n","        history_log[f'std_final_{key}'] = (sum(\n","            [(history_log[f'mean_{key}'] - x)**2 for x in final_key_values]) / n_repeats) ** 0.5  \n","    full_val_losses = [history_log[f'y_full_val_loss_run_{i}'] for i in range(n_repeats)]\n","    full_val_accuracies = [history_log[f'y_full_val_accuracy_run_{i}'] for i in range(n_repeats)]\n","    history_log['best_anytime_full_val_accuracy'] = max(full_val_accuracies)\n","    history_log['best_anytime_full_val_loss'] = min(full_val_losses)\n","    history_log['mean_full_val_loss'] = np.mean(full_val_losses)\n","    history_log['mean_full_val_accuracy'] = np.mean(full_val_accuracies)\n","        \n","    return history_log"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-11-26T10:08:10.663698Z","iopub.status.busy":"2022-11-26T10:08:10.663289Z","iopub.status.idle":"2022-11-26T10:08:10.680672Z","shell.execute_reply":"2022-11-26T10:08:10.679699Z","shell.execute_reply.started":"2022-11-26T10:08:10.663663Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def grid_search(model_build_fn,\n","                X_train,\n","                X_val,\n","                y_train=None,\n","                y_val=None,\n","                n_repeats=1, \n","                n_epochs=10, \n","                early_stopping_patience = None,\n","                verbose=0,\n","                **model_kwargs\n","               ):\n","    '''\n","    Runs fit_model for each parameter combination for those provided in model_kwargs.\n","    Provide each model_kwarg as a list of suitable objects.\n","    \n","    Args:\n","        X_train: training data\n","        X_val: validation data\n","        y_train: if X_train is not a TF dataset, then the training labels should be here\n","        y_val: if X_val is not a TF dataset, then the training labels should be here\n","        n_repeats: number of times to fit each parameter combination\n","        n_epochs: number of epochs for each fit\n","        early_stopping_patience: int, number of epochs after which fitting stops if no\n","                improvement in val_loss observed \n","        verbose: int, controls how much is printed\n","        model-kwargs: provide keyword model args each as LIST.\n","        \n","    Returns:\n","        List of dictionaries with model performance and history data\n","    '''\n","    param_combinations = []\n","    total_combs = 1\n","    number = {}\n","    for param in model_kwargs.keys():\n","        number[param] = len(model_kwargs[param])\n","        total_combs *= number[param]\n","    for i in range(total_combs):\n","        this_comb = {}\n","        cum_num=1\n","        for param in number.keys():\n","            n = int(i/cum_num)\n","            this_comb[param] = model_kwargs[param][n % number[param]]\n","            cum_num *= number[param]\n","        param_combinations.append(this_comb)\n","    history_logs = []\n","    for i, kwargs in enumerate(param_combinations):\n","        history_log = fit_model(model_build_fn,\n","                                X_train,\n","#                                 y_train,\n","                                X_val,\n","#                                 y_val,\n","                                n_repeats=n_repeats, \n","                                n_epochs=n_epochs, \n","                                early_stopping_patience = early_stopping_patience,\n","                                verbose=verbose,\n","                                model_save_suffix=f'_params_{i}',\n","                                **kwargs\n","                               )\n","        history_logs.append(history_log)\n","    return history_logs"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-11-26T10:08:10.683699Z","iopub.status.busy":"2022-11-26T10:08:10.681945Z","iopub.status.idle":"2022-11-26T10:08:10.703188Z","shell.execute_reply":"2022-11-26T10:08:10.701592Z","shell.execute_reply.started":"2022-11-26T10:08:10.683627Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def confusion_matrices(history_log, title=''):\n","    '''\n","    Plots confusion matrices based on the given history dictionary \n","    from the fit_model function\n","    '''\n","    def prob_to_pred(x):\n","        if x < 0.5:\n","            return 0\n","        else:\n","            return 1\n","        \n","    def confmat_to_plot(confmat, axis, subtitle=''):\n","        sns.heatmap(confmat, annot=True, fmt='d', cbar=False, linewidths=.5, ax=axis)\n","        axis.set_xticklabels(['Predicted Tory', 'Predicted Labour'])\n","        axis.set_yticklabels(['Actual Tory', 'Actual Labour'])\n","        axis.set_title(subtitle)\n","        \n","    # Extract predictions from log into df\n","    train_preds = { f'y_train_pred_run_{i}' : [prob_to_pred(x) for x in history_log[f'y_train_pred_run_{i}']]  for i in range(history_log['Number of runs']) }\n","    val_preds = { f'y_val_pred_run_{i}' : [prob_to_pred(x) for x in history_log[f'y_val_pred_run_{i}']]  for i in range(history_log['Number of runs']) }\n","    \n","    # Print classification reports\n","    for i,run in enumerate(train_preds.keys()):\n","        print('-----------------------')\n","        print(f'Training set, run {i}:')\n","        print('-----------------------')  \n","        print(classification_report(history_log[f'y_train_actual'], train_preds[run]))  # print classification report\n","    \n","    for i,run in enumerate(val_preds.keys()): \n","        print('-------------------------')\n","        print(f'Validation set, run {i}:')\n","        print('-------------------------')  \n","        print(classification_report(history_log[f'y_val_actual'], val_preds[run]))  # print classification report\n","        \n","    # Make plots for train + val set runs\n","    fig, ax = plt.subplots(nrows=2,ncols=history_log['Number of runs'], figsize = (4*history_log['Number of runs'], 8) )\n","    if history_log['Number of runs'] == 1:\n","        ax = ax.reshape((2,1))\n","    # Make plots for train runs\n","    for i,run in enumerate(train_preds.keys()):\n","        confmat_to_plot(confusion_matrix(history_log[f'y_train_actual'], train_preds[run]), axis=ax[0,i], subtitle=f'Training set, run {i}')\n","            \n","    # Make plots for val set runs\n","    for i,run in enumerate(val_preds.keys()):        \n","        confmat_to_plot(confusion_matrix(history_log[f'y_val_actual'], val_preds[run]), axis=ax[1,i], subtitle=f'Validation set, run {i}')\n","    plt.suptitle(title)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-11-26T10:08:10.706133Z","iopub.status.busy":"2022-11-26T10:08:10.705053Z","iopub.status.idle":"2022-11-26T10:08:10.722778Z","shell.execute_reply":"2022-11-26T10:08:10.720895Z","shell.execute_reply.started":"2022-11-26T10:08:10.706065Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def visualize_log(history_log_list):\n","    '''\n","    Visualizes the history of the grid search.\n","    \n","    Args:\n","        history_log_list : list of data from grid search\n","        input_format: use 'tf' for tf.data.Dataset input, 'pd' for pandas dataframe.\n","    '''\n","    # Establish the df and display parameters for reader\n","    print('-----------------------\\nParameter lookup table:\\n-----------------------')\n","    log_df = pd.DataFrame(history_log_list)\n","    log_df.index.name = 'Parameter Index'\n","    params_cols = set()\n","    for i in range(len(log_df)):\n","        params_cols = params_cols.union(set(log_df.loc[i,'Parameter keys']))\n","    other_cols = ['trainable parameters','mean_val_accuracy','best_anytime_val_accuracy', 'Mean run time (mins)',\n","                 'best_anytime_full_val_loss','best_anytime_full_val_accuracy',\n","                 'mean_full_val_loss','mean_full_val_accuracy', 'Val_loss mean trend at end',\n","                 'Number of runs', 'Number of epochs']\n","    display(log_df[list(params_cols)+other_cols])\n","    log_df  = log_df.reset_index()\n","    log_df.to_csv('log_df.csv')\n","    \n","    # Bar charts with best accuracies    \n","    fig = px.bar(data_frame=log_df, x=range(len(log_df)), y='mean_val_accuracy', hover_name='Parameter Index', title='Comparing mean validation accuracies')\n","    fig.show()\n","    fig = px.bar(data_frame=log_df, x=range(len(log_df)), y='best_anytime_val_accuracy', hover_name='Parameter Index', title='Comparing optimum validation accuracies')\n","    fig.show()\n","    \n","    # Loss plots\n","    def metric_df(metric='loss', run=0):\n","        padded_losses_dict = {}\n","        length = max([len(log_df.loc[i,f'run_{run}_{metric}']) for i in range(len(log_df))])\n","        for i in range(len(log_df)):\n","            this_loss = log_df.loc[i,f'run_{run}_{metric}']\n","            while len(this_loss) < length:\n","                this_loss.append(np.NaN)\n","            padded_losses_dict[log_df.loc[i,'Parameter Index']] = this_loss\n","        return pd.DataFrame( data=padded_losses_dict )\n","    \n","    for metric in ['loss', 'val_loss', 'accuracy', 'val_accuracy']:\n","        for run in range(log_df['Number of runs'].min()):\n","            fig = px.line(data_frame=metric_df(metric=metric, run=run),\n","                    title=f'Curves for {metric} of run {run} for each parameter set'\n","                   )\n","            fig.update_layout(legend_title_text='Parameter index')\n","            fig.show()\n","    \n","    # Show confusion matrices\n","    for history_log in history_log_list:\n","        confusion_matrices(history_log, \n","                           title=f'Confusion matrix for {history_log[\"Parameters\"]}'\n","                          )        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-26T10:08:08.204102Z","iopub.status.busy":"2022-11-26T10:08:08.203785Z","iopub.status.idle":"2022-11-26T10:08:08.215269Z","shell.execute_reply":"2022-11-26T10:08:08.213432Z","shell.execute_reply.started":"2022-11-26T10:08:08.204073Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# This function is for preparing the data for testing the model performance \n","# on the full speeches from the validation set. This will mimic the planned\n","# actual usage.\n","\n","def full_val_samples(overlap = 50):\n","    '''\n","    Loads data into tf datasets:\n","    * train, val and test sets from the sampled speeches.\n","    * for each full speech in the val set, also get a dataset of samples from it.\n","    Speech segments are tokenized.\n","    \n","    Args:\n","        batch_size: int, batch size for training\n","        max_length: int, max length of a tokenized sequence\n","        overlap: int, amount of overlap between samples for the full val set\n","        \n","    Returns:\n","        4x datasets: train, val and test sets, and segmented full val speeches\n","        1x pandas dataframe of segmented full val speeches, with index        \n","    '''\n","    full_val_df = pd.read_csv('../input/speeches-sampling/full_val.csv', index_col=0)\n","    full_X_val = full_val_df['Speech']\n","    full_y_val = full_val_df['Label']\n","    speech_segments = []\n","    labels = []\n","    speech_indices = []\n","    for index, speech_label in enumerate(zip(full_X_val, full_y_val)):\n","        speech = speech_label[0]\n","        label = speech_label[1]\n","        split_speech = speech.split(' ')\n","        length = len(split_speech)\n","        if length < 100:\n","            segments = [speech]\n","        else:\n","            segments = []\n","            i=0\n","            while i <= length - 100:\n","                segments.append(split_speech[i:i+100])\n","                i+=(100-overlap)\n","        speech_segments += segments\n","        labels += [label]*len(segments)\n","        speech_indices += [index]*len(segments)\n","    speech_segments_df = pd.DataFrame({'Speech index':speech_indices,\n","                              'Speech': speech_segments,\n","                              \"Label\":labels\n","                             })\n","    speech_segments_ds = tokenize_to_ds(speech_segments_df, batch_size=512)\n","    \n","    return speech_segments_ds, speech_segments_df"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-11-26T10:08:10.725087Z","iopub.status.busy":"2022-11-26T10:08:10.724640Z","iopub.status.idle":"2022-11-26T10:08:10.738422Z","shell.execute_reply":"2022-11-26T10:08:10.737599Z","shell.execute_reply.started":"2022-11-26T10:08:10.725045Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def val_performance(model, speech_segments_ds, speech_segments_df):\n","    '''\n","    Measures the perfomance of the given model on the full length speeches from\n","    the validation set.\n","    \n","    Args:\n","        model: the model object\n","        speech_segments_ds: tf dataset with the segmented full speeches (as output by\n","                the function full_val_samples)\n","        speech_segments_df: pandas dataframe with the same segmented full speeches \n","                (as output by the function full_val_samples)\n","    \n","    Returns:\n","        float: loss\n","        float: accuracy\n","    '''\n","    # get predicted proba for each segment\n","    segment_probs = model.predict(speech_segments_ds)\n","    # add to dataframe\n","    speech_segments_df['Probability'] = segment_probs\n","    # get mean prediction proba for each speech\n","    pred_df = speech_segments_df.groupby('Speech index')[['Label','Probability']].mean()\n","    # get predicted label\n","    pred_df['Prediction'] = pred_df['Probability'].apply(lambda x: 0 if x<0.5 else 1)\n","    # get loss and accuracy\n","    loss = log_loss(pred_df['Label'], pred_df['Probability'])\n","    accuracy = accuracy_score(pred_df['Label'], pred_df['Prediction'])\n","    return loss, accuracy"]},{"cell_type":"markdown","metadata":{},"source":["# Model architecture<a id='architecture'></a>\n","\n","The model is built in the function `build_model` below. The model can be customized with various arguments, but the general structure is as follows:\n","\n","* **Input**: The model expects the input to be sequences of 100 integers. Speeches/speech segments should be tokenized first before being plugged into the model. It is possible to include this sete inside the model with a TextVectorization layer, however these layers cannot be exported to TensorFlow.js (as we intend to do).\n","* **Embedding layer**: the input sequences are embedded into $n$-dimensional space. You may choose $n$, and you may also try using embeddings pretrained using GloVe (for `embed_dim` enter either \"glove 50\" or \"glove 50 trainable\" for the 50 dimensional embeddings, making this layer either trainable or not. GloVe embeddings may be 50, 100, 200 or 300 dimensional.)\n","* **Convolutional layers**: the embedded strings are passed through a sequence of three 1-dimensional convolutional layers. There is a skip connection (so we can propagate through three Conv1D layers or just one layer). The number of filters and filter size can be chosen with keywords `Conv_filters` and `Conv_filter_size`. Batch normalization is applied between each convultion layer and applying the activation function.\n","* **LSTM layers**: the user can specify the number of LSTM layers with the keyword `LSTM_layers`, whether they are bidirectional, with `BRNN` (a boolean), the numebr of units in each layer, `LSTM_units`, and what regularization to use: `LSTM_kernel_reg` requires an initialized keras regularizer (e.g. l1 or l2 - we used l2), `LSTM_dropout` expects a float between 0 and 1.\n","* **Dense layers to output**: before being passed through a single unit dense layer for the final output, we pass the data through some Dense layers of diminishing dimension. The initial dimension is given by `Dense_units`, the dimension is halved in each successive layer, with a total number of layers (excluding the final output layer) given by `Dense_layers`. Make `Dense_units` a power of 2, large enough so that each layer has an integer dimension! Regularization is applied through dropout layers.\n","* **Dropout layers**: dropout layers are scattered throughout the model, with a dropout rate controlled by `Dropout_rate`.\n","\n","The model is compiled using an Adam optimizer, with a learning rate given by `learning_rate`.\n","\n","If using the GloVe embeddings, then these need to be initialized too. Use the `get_glove_embedding_layer` to do this, which returns an embedding layer with pretrained weights.\n","\n","Finally, there is an option to load an existing model for further training (useful when training takes more than 12 hours permitted on Kaggle servers). Set `saved_model_path` to the model path and all other parameters are ignored."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-11-26T10:08:10.741189Z","iopub.status.busy":"2022-11-26T10:08:10.739666Z","iopub.status.idle":"2022-11-26T10:08:10.755179Z","shell.execute_reply":"2022-11-26T10:08:10.754240Z","shell.execute_reply.started":"2022-11-26T10:08:10.741119Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def get_glove_embedding_layer(vocab, embed_dim=300, trainable=False):\n","    '''\n","    Creates an embvedding layer using GloVe emebedding vectors.\n","    \n","    Args:\n","        vocab: dictionary of word to index for the tokenizer\n","        embed_dim: int, dimension of the embedding\n","        trainable: bool, whether the layer is trainable or not\n","        \n","    Returns:\n","        Embedding layer\n","    '''\n","    # First get the vectors            \n","    glove_dict = {}\n","    if embed_dim not in [50,100,200,300]:\n","        raise ValueError('Glove embedding dimensions are 50, 100, 200 or 300 only.')\n","    filename = f'../input/glove6b/glove.6B.{embed_dim}d.txt'\n","    with open(filename) as file:\n","        for line in file:\n","            values = line.split()\n","            word = values[0]\n","            vector = values[1:]\n","            glove_dict[word] = np.asarray(vector, dtype='float32')\n","    # Get vocab dict, word to index, from tokenizer\n","    vocab_dict = vocab\n","    # Next create the matrix\n","    embedding_matrix = np.zeros((len(vocab_dict)+1, embed_dim))\n","    for word, index in vocab_dict.items():\n","        embedding_vector = glove_dict.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[index] = embedding_vector\n","    # Now create and return the layer\n","    return layers.Embedding(input_dim = len(vocab_dict)+1,\n","                     output_dim = embed_dim,\n","                     trainable = trainable,\n","                     weights = [embedding_matrix]\n","                    )"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-11-26T10:13:47.714065Z","iopub.status.busy":"2022-11-26T10:13:47.713681Z","iopub.status.idle":"2022-11-26T10:13:47.736780Z","shell.execute_reply":"2022-11-26T10:13:47.735756Z","shell.execute_reply.started":"2022-11-26T10:13:47.714028Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def build_model(embed_dim = 32,\n","               Conv_filters=None,\n","               Conv_filter_size=3,\n","               LSTM_layers=2, \n","               LSTM_units=64, \n","               LSTM_kernel_reg=None,\n","               LSTM_dropout=0,\n","               BRNN=False,\n","               Dense_layers=1, \n","               Dense_units=128, \n","               Dropout_rate=0,\n","               learning_rate=0.001,\n","               saved_model_path=None                \n","      ):\n","    '''\n","    Builds and compiles neural network.\n","    \n","    Args:\n","        embed_dim: int or string. If int then created embedding layer with this\n","                dimension. If string should either be 'glove 50' or 'glove 50 \n","                trainable', where 50 can be replaced by 100, 200 or 300 and\n","                refers to the dimension of the embedding using the GloVe\n","                word embeddings.\n","        Conv_filters: int, number of filters in each convolutional layer\n","        Conv_filter_size: int, sise of each filter\n","        LSTM_layers: int, number of LSTM layers \n","        LSTM_units: int, number of the units per LSTM layer\n","        LSTM_kernel_reg: initialized keras regularizer, or None\n","        LSTM_dropout: float, between 0 and 1. Dropout rate used in LSTM layers\n","        BRNN: bool, whether to use bidirectional LSTM layers or not\n","        Dense_layers: int, number of dense layers\n","        Dense_units: int, dimension of first dense layer. Following dense layer\n","                dimensions are halved each time\n","        Dropout_rate: float, between 0 and 1. Dropout rate for all non-LSTM\n","                dropout layers.\n","        learning_rate: float, determines the learning rate\n","        saved_model_path: None or string with model path. If None, then no model\n","                is loaded. If model path provided then all other params are\n","                ignored.        \n","        \n","    Returns:\n","        compiled model\n","    '''\n","    if load_saved_model != None:\n","        model = tf.keras.models.load_model(saved_model_path)\n","        return model\n","    \n","    string_vec = tf.keras.Input(shape=(100,), dtype=tf.int32, name='input_layer')\n","    \n","    \n","    \n","    # create embedding layer\n","    if isinstance(embed_dim, int):\n","        embed_layer = layers.Embedding(vocab_size+1, embed_layer)\n","    else:\n","        if embed_dim in [f'glove {x}' for x in [50,100,200,300]]:\n","            glove_dim = int(re.findall(r'\\d+', embed_dim)[0])\n","            embed_layer = get_glove_embedding_layer(vocab, embed_dim=glove_dim, trainable=False)\n","        elif embed_dim in [f'glove {x} trainable' for x in [50,100,200,300]]:\n","            glove_dim = int(re.findall(r'\\d+', embed_dim)[0])\n","            embed_layer = get_glove_embedding_layer(vocab, embed_dim=glove_dim, trainable=True)\n","        else:\n","            raise ValueError('Embedding layer format not recognized. \\\n","                             Should be either integer for trainable embedding of that dimension, \\\n","                             or either \"glove x\" or \"glove x trainable\", for x = 50, 100, 200, 300.')\n","        \n","        embed_dim = glove_dim\n","            \n","    x = embed_layer(string_vec)\n","            \n","    x = layers.Dropout(Dropout_rate)(x)\n","    \n","    if Conv_filters != None:\n","        x1 = layers.Conv1D(filters=Conv_filters, kernel_size=Conv_filter_size, padding = 'same', activation=None)(x)\n","        x1 = layers.BatchNormalization()(x1)\n","        x1 = layers.Activation('relu')(x1)\n","        x1 = layers.Conv1D(filters=Conv_filters, kernel_size=Conv_filter_size, padding = 'same', activation=None)(x1)\n","        x1 = layers.BatchNormalization()(x1)\n","        x1 = layers.Activation('relu')(x1)\n","        x1 = layers.Conv1D(filters=Conv_filters, kernel_size=Conv_filter_size, padding = 'same', activation=None)(x1)\n","        x1 = layers.BatchNormalization()(x1)\n","        # skip connection\n","        x2 = layers.Conv1D(filters=Conv_filters, kernel_size=Conv_filter_size, padding = 'same', activation=None)(x)\n","        x2 = layers.BatchNormalization()(x2)\n","        # add\n","        x = layers.Add()([x1,x2])\n","        x = layers.Activation('relu')(x)\n","    \n","    # add LSTM layers\n","    if BRNN:\n","        if LSTM_layers>1:\n","            for i in range(LSTM_layers-1):\n","                x = layers.Bidirectional(layers.LSTM(LSTM_units, \n","                                                     return_sequences = True, \n","                                                     kernel_regularizer = LSTM_kernel_reg, \n","                                                     recurrent_dropout=LSTM_dropout, \n","                                                     name = f'LSTM_{i}'))(x)\n","        else:\n","            i=0\n","        x = layers.Bidirectional(layers.LSTM(LSTM_units, recurrent_dropout=LSTM_dropout, name = f'LSTM_{i+1}'))(x)\n","        \n","    else:\n","        if LSTM_layers>1:\n","            for i in range(LSTM_layers-1):\n","                x = layers.LSTM(LSTM_units, \n","                                return_sequences = True, \n","                                kernel_regularizer = LSTM_kernel_reg, \n","                                recurrent_dropout=LSTM_dropout, \n","                                name = f'LSTM_{i}')(x)\n","        else:\n","            i=0\n","        x = layers.LSTM(LSTM_units, recurrent_dropout=LSTM_dropout, name = f'LSTM_{i+1}')(x)\n","        \n","    # Another dropout layer\n","    x = layers.Dropout(Dropout_rate)(x)\n","    \n","    # we pass it through a few dense layers, with diminishing dimension, to get the output\n","    for i in range(Dense_layers):\n","        x = layers.Dense(Dense_units/(2**i), activation = 'relu')(x)\n","        x = layers.Dropout(Dropout_rate)(x)\n","        \n","    output = layers.Dense(1, activation = 'sigmoid')(x)\n","\n","    model = tf.keras.Model(inputs = string_vec, outputs = output, name='Transformer_encoder')\n","    \n","    # compile\n","    Adam = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(loss='binary_crossentropy', optimizer=Adam, metrics=['accuracy'])\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["# The grid search begins\n","\n","We build the model, display a summary with one set of parameters, and then fit it using all combinations of those given in the `grid_search` function.  Finally, performance data is given and visualized after fitting is completed."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-26T10:08:08.217598Z","iopub.status.busy":"2022-11-26T10:08:08.217143Z","iopub.status.idle":"2022-11-26T10:08:10.633985Z","shell.execute_reply":"2022-11-26T10:08:10.632903Z","shell.execute_reply.started":"2022-11-26T10:08:08.217557Z"},"trusted":true},"outputs":[],"source":["# Load data for full speech validation tests.\n","speech_segments_ds, speech_segments_df = full_val_samples()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-26T10:13:56.775663Z","iopub.status.busy":"2022-11-26T10:13:56.775066Z","iopub.status.idle":"2022-11-26T10:14:08.336371Z","shell.execute_reply":"2022-11-26T10:14:08.334857Z","shell.execute_reply.started":"2022-11-26T10:13:56.775632Z"},"trusted":true},"outputs":[],"source":["# Load the model and view a summary.\n","model = build_model(\n","    embed_dim = 'glove 50 trainable',\n","    Conv_filters=128,\n","    Conv_filter_size=5,\n","    LSTM_layers=3, \n","    LSTM_units=64, \n","    LSTM_kernel_reg=None,\n","    LSTM_dropout=0.3,\n","    BRNN=True,\n","    Dense_layers=3, \n","    Dense_units=128, \n","    Dropout_rate=0.3,\n","    learning_rate = 0.00001\n","#     load_saved_model = '/kaggle/input/speeches-classification-model-trials-v5/model_params_0_run_0'\n",")\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-11-26T10:14:43.788827Z","iopub.status.busy":"2022-11-26T10:14:43.787423Z"},"trusted":true},"outputs":[],"source":["reg = l2(0.01)\n","reg2 = l2(0.1)\n","n_repeats = 1\n","history_log = grid_search(build_model, train_ds, val_ds, n_repeats = n_repeats, \n","                          n_epochs=18, early_stopping_patience=8, verbose=1,\n","                                        embed_dim = ['glove 50 trainable'],\n","                                        Conv_filters=[128],\n","                                        Conv_filter_size=[5],\n","                                        LSTM_layers=[3], \n","                                        LSTM_units=[64], \n","                                        LSTM_kernel_reg=[reg],\n","                                        LSTM_dropout=[0.3],\n","                                        BRNN=[True],\n","                                        Dense_layers=[3], \n","                                        Dense_units=[128], \n","                                        Dropout_rate=[0.3],\n","                                        learning_rate = [0.00015]\n","#                                               load_saved_model = ['/kaggle/input/speeches-classification-model-trials-v5/model_params_0_run_0']\n","                                  )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-11-22T19:51:20.547876Z","iopub.status.idle":"2022-11-22T19:51:20.5489Z","shell.execute_reply":"2022-11-22T19:51:20.548674Z","shell.execute_reply.started":"2022-11-22T19:51:20.548645Z"},"trusted":true},"outputs":[],"source":["visualize_log(history_log)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 ('.venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"48cace39d0b04255ae78003876d64bc184d212be70cd059ed579da6df7df9f1a"}}},"nbformat":4,"nbformat_minor":4}
